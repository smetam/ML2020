{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайные леса\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "В этом задании вам предстоит реализовать ансамбль деревьев решений, известный как случайный лес, применить его к публичным данным и сравнить его эффективность с ансамблем, предоставляемым библиотекой CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.utils import shuffle\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 (3 балла)\n",
    "Реализуем сам Random Forest. Идея очень простая: строим `n` деревьев, а затем берем модальное предсказание.\n",
    "\n",
    "#### Параметры конструктора\n",
    "`n_estimators` - количество используемых для предсказания деревьев.\n",
    "\n",
    "Остальное - параметры деревьев.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - строит `n_estimators` деревьев по выборке `X`.\n",
    "\n",
    "`predict(X)` - для каждого элемента выборки `X` возвращает самый частый класс, который предсказывают для него деревья."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\", n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_classes = 0\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.trees = []\n",
    "        self.tree_idx = []\n",
    "        for _ in range(n_estimators):\n",
    "            classifier = tree.DecisionTreeClassifier(\n",
    "                criterion=criterion, \n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                max_depth=max_depth,\n",
    "                max_features=max_features\n",
    "            )\n",
    "            self.trees.append(classifier)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(set(y))\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        n_items = y.size\n",
    "        for i in range(self.n_estimators):\n",
    "            idx = np.random.choice(n_items, n_items)\n",
    "            self.tree_idx.append(idx)\n",
    "            X_temp, y_temp = X[idx], y[idx]\n",
    "            self.trees[i].fit(X_temp, y_temp)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_estimators), dtype=np.int64)\n",
    "        for i in range(self.n_estimators):\n",
    "            predictions[:, i] = self.trees[i].predict(X)\n",
    "        counts = np.apply_along_axis(lambda x: np.bincount(x, minlength=self.n_classes), axis=1, arr=predictions)\n",
    "        return np.apply_along_axis(lambda x: np.argmax(x), axis=1, arr=counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 (3 балла)\n",
    "Оптимизируйте по AUC на кроссвалидации (размер валидационной выборки - 20%) параметры своей реализации Random Forest: \n",
    "\n",
    "максимальную глубину деревьев из [2, 3, 5, 7, 10], количество деревьев из [5, 10, 20, 30, 50, 100]. \n",
    "\n",
    "Постройте ROC кривую (и выведите AUC и accuracy) для лучшего варианта.\n",
    "\n",
    "Подсказка: можно построить сразу 100 деревьев глубины 10, а потом убирать деревья и\n",
    "глубину."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_scale(X):\n",
    "    for col in list(X):\n",
    "        mean = X[col].mean()\n",
    "        std = X[col].std()\n",
    "        X[col] = (X[col] - mean) / std\n",
    "    return X\n",
    "\n",
    "def read_cancer_dataset(path_to_csv):\n",
    "    # Возвращает пару из X и y. X - массив векторов. y - соответствующие векторам метки\n",
    "    df = shuffle(pd.read_csv(path_to_csv))\n",
    "    y = (df['label'].values == \"M\").astype(int)\n",
    "    X = df.drop('label', axis=1)\n",
    "    X = std_scale(X).values\n",
    "    return X, y\n",
    "\n",
    "def read_spam_dataset(path_to_csv):\n",
    "    # Возвращает пару из X и y. X - массив векторов. y - соответствующие векторам метки\n",
    "    df = shuffle(pd.read_csv(path_to_csv))\n",
    "    y = df['label'].values\n",
    "    X = df.drop('label', axis=1)\n",
    "    X = std_scale(X).values\n",
    "    return X, y\n",
    "\n",
    "def crossvalidation_train_test_split(X, y, ratio=0.2):\n",
    "    # Возвращает X_train, y_train, X_test, y_test\n",
    "    n_items = len(y)\n",
    "    n = int(n_items * ratio)\n",
    "    start, stop = 0, n\n",
    "    while stop <= n_items:\n",
    "        test_idx = np.arange(start, stop)\n",
    "        yield (X[~test_idx], y[~test_idx], X[test_idx], y[test_idx])\n",
    "        start, stop = stop, stop + n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для max_depth = 10, n_estimators = 100 roc_auc score: 0.9508621164833662.\n",
      "Для max_depth = 10, n_estimators = 50 roc_auc score: 0.9501725395437892.\n",
      "Для max_depth = 10, n_estimators = 30 roc_auc score: 0.9509271805932327.\n",
      "Для max_depth = 7, n_estimators = 100 roc_auc score: 0.9522314435569186.\n",
      "Для max_depth = 7, n_estimators = 50 roc_auc score: 0.9520660401872899.\n",
      "Для max_depth = 5, n_estimators = 100 roc_auc score: 0.9557433483188232.\n",
      "Для max_depth = 5, n_estimators = 50 roc_auc score: 0.9554455779089327.\n",
      "Лучший выбор для max_depth, n_estimators = (5, 50).\n"
     ]
    }
   ],
   "source": [
    "X, y = read_cancer_dataset(\"hw1/cancer.csv\")\n",
    "\n",
    "auc_limit = 0.95\n",
    "\n",
    "depth_choices = [2, 3, 5, 7, 10]\n",
    "num_trees = [5, 10, 20, 30, 50, 100]\n",
    "\n",
    "best_choice = 10, 100\n",
    "\n",
    "for depth in reversed(depth_choices):\n",
    "    for n_trees in reversed(num_trees):\n",
    "        forest = RandomForestClassifier(max_depth=depth, n_estimators=n_trees)\n",
    "        scores = []\n",
    "        for X_train, y_train, X_test, y_test in crossvalidation_train_test_split(X, y):\n",
    "            forest.fit(X_train, y_train)\n",
    "            y_predict = forest.predict(X_test)\n",
    "            score = metrics.roc_auc_score(y_test, y_predict)\n",
    "            scores.append(score)\n",
    "        total_score = sum(scores) / len(scores) \n",
    "        if total_score > auc_limit:\n",
    "            best_choice = depth, n_trees\n",
    "        else:\n",
    "            break\n",
    "        print(f'Для max_depth = {depth}, n_estimators = {n_trees} roc_auc score: {total_score}.')\n",
    "print(f'Лучший выбор для max_depth, n_estimators = {best_choice}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для max_depth = 10, n_estimators = 100 roc_auc score: 0.9310182716819639.\n",
      "Для max_depth = 10, n_estimators = 50 roc_auc score: 0.9295795034509986.\n",
      "Для max_depth = 10, n_estimators = 30 roc_auc score: 0.9292286508060255.\n",
      "Для max_depth = 10, n_estimators = 20 roc_auc score: 0.9268269512614408.\n",
      "Для max_depth = 7, n_estimators = 100 roc_auc score: 0.9212141827236746.\n",
      "Лучший выбор для max_depth, n_estimators = (7, 100).\n"
     ]
    }
   ],
   "source": [
    "X, y = read_spam_dataset(\"hw1/spam.csv\")\n",
    "\n",
    "auc_limit = 0.92\n",
    "\n",
    "depth_choices = [2, 3, 5, 7, 10]\n",
    "num_trees = [5, 10, 20, 30, 50, 100]\n",
    "\n",
    "best_choice = 10, 100\n",
    "best_auc = 0\n",
    "\n",
    "for depth in reversed(depth_choices):\n",
    "    for n_trees in reversed(num_trees):\n",
    "        forest = RandomForestClassifier(max_depth=depth, n_estimators=n_trees)\n",
    "        scores = []\n",
    "        for X_train, y_train, X_test, y_test in crossvalidation_train_test_split(X, y):\n",
    "            forest.fit(X_train, y_train)\n",
    "            y_predict = forest.predict(X_test)\n",
    "            score = metrics.roc_auc_score(y_test, y_predict)\n",
    "            scores.append(score)\n",
    "        total_score = sum(scores) / len(scores) \n",
    "        if total_score > auc_limit:\n",
    "            best_choice = depth, n_trees\n",
    "        else:\n",
    "            break\n",
    "        print(f'Для max_depth = {depth}, n_estimators = {n_trees} roc_auc score: {total_score}.')\n",
    "print(f'Лучший выбор для max_depth, n_estimators = {best_choice}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4 (3 балла)\n",
    "Часто хочется понимать, насколько большую роль играет тот или иной признак для предсказания класса объекта. Есть различные способы посчитать его важность. Один из простых способов сделать это для Random Forest выглядит так:\n",
    "1. Посчитать out-of-bag ошибку предсказания `err_oob` (https://en.wikipedia.org/wiki/Out-of-bag_error)\n",
    "2. Перемешать значения признака `j` у объектов выборки (у каждого из объектов изменится значение признака `j` на какой-то другой)\n",
    "3. Посчитать out-of-bag ошибку (`err_oob_j`) еще раз.\n",
    "4. Оценкой важности признака `j` для одного дерева будет разность `err_oob_j - err_oob`, важность для всего леса считается как среднее значение важности по деревьям.\n",
    "\n",
    "Реализуйте функцию `feature_importance`, которая принимает на вход Random Forest и возвращает массив, в котором содержится важность для каждого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_feature(X, feature):\n",
    "    X_temp = copy.deepcopy(X)\n",
    "    t = copy.deepcopy(X[:, feature])\n",
    "    np.random.shuffle(t)\n",
    "    X_temp[:, feature] = t\n",
    "    return X_temp\n",
    "\n",
    "\n",
    "def feature_importance(rfc):\n",
    "    err_oob = []\n",
    "    err_oob_feature = {feature:[] for feature in range(rfc.X.shape[1])}\n",
    "    predictions = np.zeros((rfc.y.size, rfc.n_estimators), dtype=np.int64)\n",
    "    for i in range(rfc.n_estimators):\n",
    "        idx = ~np.unique(rfc.tree_idx[i])\n",
    "        y_true = rfc.y[idx]\n",
    "        y_pred = rfc.predict(rfc.X[idx])\n",
    "        err_oob.append(1 - np.sum(y_true == y_pred) / y_true.size)\n",
    "        \n",
    "        for feature in range(rfc.X.shape[1]):\n",
    "            X = shuffle_feature(rfc.X, feature)\n",
    "            y_pred = rfc.predict(X[idx])\n",
    "            err_oob_feature[feature].append(1 - np.sum(y_true == y_pred) / y_true.size)\n",
    "    \n",
    "    err_oob = np.array(err_oob)\n",
    "    \n",
    "    imp = [np.mean(np.array(err_oob_feature[feature]) - err_oob)  for feature in range(rfc.X.shape[1])]\n",
    "    return imp\n",
    "\n",
    "def most_important_features(importance, names, k=20):\n",
    "    # Выводит названия k самых важных признаков\n",
    "    idicies = np.argsort(importance)[::-1][:k]\n",
    "    return np.array(names)[idicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте решение на простом синтетическом наборе данных. В результате должна получиться точность `1.0`, наибольшее значение важности должно быть у признака с индексом `4`, признаки с индексами `2` и `3`  должны быть одинаково важны, а остальные признаки - не важны совсем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Importance: [0.0, 0.0, 0.19269400987902305, 0.19577865157868188, 0.44336303519507886, 0.0]\n"
     ]
    }
   ],
   "source": [
    "def synthetic_dataset(size):\n",
    "    X = [(np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3, \n",
    "          i % 6 == 0, i % 3 == 2, np.random.randint(0, 2)) for i in range(size)]\n",
    "    y = [i % 3 for i in range(size)]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = synthetic_dataset(1000)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X, y)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X) == y))\n",
    "print(\"Importance:\", feature_importance(rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте, какие признаки важны для датасетов cancer и spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9269723973049337\n",
      "Importance: [0.0, 0.00017787327314991775, 0.0004259789379414558, -6.823609689525778e-06, 0.003930980977566076, 0.0009407792135339488, 0.01857384150825921, 0.0018499292162056124, 0.0002477615879583728, -3.46185232234042e-05, 0.0003662742508151995, 3.5154709732372335e-05, -6.896551724138167e-06, 0.00024006061185428252, 0.0, 0.008233544557439267, 0.0007086716179847219, 0.0008537557587013955, 0.0020538804235559647, 0.0009145804971579241, 0.0033617571218111063, -2.2720809420784249e-07, 0.0013893940762475876, -0.0005506538862923094, 0.013663416420035497, 0.0018779579726150497, 0.007817572947488347, 0.00026842471572644075, 0.0002478276544446967, 0.00013117822238095656, 1.3783667539075672e-05, 0.00011681694766104034, 2.0763789916187213e-05, 2.0602081553189322e-05, 0.00032308918949283563, 3.4257467708627145e-05, 0.0020982225876847904, 0.0, 0.00030946122929178757, 0.0, 0.0, 0.00035774615810545195, 8.22886741014428e-05, 1.3708251023538676e-05, 0.0006191037912214425, 0.002054638561096411, 2.077956869668718e-05, 0.0001856422982876982, 0.0, 0.00040517799489165897, 0.00011697477003813183, 0.020895917813971678, 0.017138002854320426, -7.639547729167972e-05, 0.008755924025515317, 0.004894083442841008, 0.004299480776216751]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['char_freq_!', 'word_freq_remove', 'char_freq_$', 'word_freq_hp',\n",
       "       'capital_run_length_average', 'word_freq_free', 'word_freq_george',\n",
       "       'capital_run_length_longest', 'capital_run_length_total',\n",
       "       'word_freq_our'], dtype='<U26')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = read_spam_dataset(\"hw1/spam.csv\")\n",
    "spam_feature_names = list(pd.read_csv(\"hw1/spam.csv\"))[:-1]\n",
    "rfc = RandomForestClassifier(max_depth=5, n_estimators=50)\n",
    "rfc.fit(X, y)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X) == y))\n",
    "imp = feature_importance(rfc)\n",
    "print(\"Importance:\", imp)\n",
    "most_important_features(imp, spam_feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9929701230228472\n",
      "Importance: [0.0, 0.0030475444894176727, 0.0012804493364841551, -0.0003872875491544403, 0.0, 0.0, 0.001996671572054496, 0.004713879259517082, 0.00016653313949446468, 0.0007775880176099026, 0.0027756406644949805, 0.0, 0.0, 0.004110224727681131, 0.002000372718111347, 0.0, 0.0022289575092931035, 0.0016088339818197794, 0.0, 0.00010901205964474636, 0.0034943293092831486, 0.006278219860728899, 0.004225388142025401, 0.00598286685114092, 0.0017194876444462225, 0.00044357117578173175, 0.004614506711497022, 0.014042632016168068, 0.0007747630522579429, 0.0006647360791259471]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['28', '22', '24', '8', '27', '23', '14', '21', '2', '11'],\n",
       "      dtype='<U2')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = read_cancer_dataset(\"hw1/cancer.csv\")\n",
    "cancer_feature_names = list(pd.read_csv(\"hw1/cancer.csv\"))[1:]\n",
    "rfc = RandomForestClassifier(max_depth=5, n_estimators=50)\n",
    "rfc.fit(X, y)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X) == y))\n",
    "imp = feature_importance(rfc)\n",
    "print(\"Importance:\", imp)\n",
    "most_important_features(imp, cancer_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5 (1 балл)\n",
    "В качестве аьтернативы попробуем CatBoost. \n",
    "\n",
    "Туториалы можно найти, например, [здесь](https://catboost.ai/docs/) и [здесь](https://github.com/catboost/tutorials/blob/master/python_tutorial.ipynb).\n",
    "\n",
    "Также, как и реализованный ними RandomForest, примените его для наших датасетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для CatBoostClassifier c дефолтными параметрами roc_auc score: 0.9503852817701646.\n"
     ]
    }
   ],
   "source": [
    "X, y = read_spam_dataset(\"hw1/spam.csv\")\n",
    "\n",
    "cat_forest = CatBoostClassifier(logging_level='Silent')\n",
    "scores = []\n",
    "for X_train, y_train, X_test, y_test in crossvalidation_train_test_split(X, y):\n",
    "    cat_forest.fit(X_train, y_train)\n",
    "    y_predict = cat_forest.predict(X_test)\n",
    "    score = metrics.roc_auc_score(y_test, y_predict)\n",
    "    scores.append(score)\n",
    "total_score = sum(scores) / len(scores) \n",
    "print(f'Для CatBoostClassifier c дефолтными параметрами roc_auc score: {total_score}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для CatBoostClassifier c дефолтными параметрами roc_auc score: 0.9622937789997751.\n"
     ]
    }
   ],
   "source": [
    "X, y = read_cancer_dataset(\"hw1/cancer.csv\")\n",
    "\n",
    "cat_forest = CatBoostClassifier(logging_level='Silent')\n",
    "scores = []\n",
    "for X_train, y_train, X_test, y_test in crossvalidation_train_test_split(X, y):\n",
    "    cat_forest.fit(X_train, y_train)\n",
    "    y_predict = cat_forest.predict(X_test)\n",
    "    score = metrics.roc_auc_score(y_test, y_predict)\n",
    "    scores.append(score)\n",
    "total_score = sum(scores) / len(scores) \n",
    "print(f'Для CatBoostClassifier c дефолтными параметрами roc_auc score: {total_score}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
